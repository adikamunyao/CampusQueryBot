{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200dbdca-af9b-4aa5-817b-55438179b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import requests\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd262896-0b63-4ef3-8c96-4496444f1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79fee919-0fb3-4c10-b103-115b9b4d20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    return '\\n'.join(line for line in lines if line)\n",
    "\n",
    "def fetch_sitemap(url):\n",
    "    r = requests.get(url)\n",
    "    xml = r.text\n",
    "    raw = xmltodict.parse(xml)\n",
    "    return raw\n",
    "\n",
    "def get_relevant_pages(sitemap, keyword):\n",
    "    pages = []\n",
    "    for info in sitemap['urlset']['url']:\n",
    "        url = info['loc']\n",
    "        if keyword in url:\n",
    "            pages.append({'text': extract_text_from(url), 'source': url})\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460799b-1ada-4d65-9015-0853ccaf50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "sitemap_url = \"https://www.rgu.ac.uk/index.php?option=com_jmap&view=sitemap&format=xml\"\n",
    "sitemap = fetch_sitemap(sitemap_url)\n",
    "pages = get_relevant_pages(sitemap, 'international-students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cc439-986d-4b1f-b24e-957a2dfc9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0ffe8-1537-48a7-ba54-deabf9bf85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(pages):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
    "    docs, metadatas = [], []\n",
    "    for page in pages:\n",
    "        splits = text_splitter.split_text(page['text'])\n",
    "        docs.extend(splits)\n",
    "        metadatas.extend([{\"source\": page['source']}] * len(splits))\n",
    "    return docs, metadatas\n",
    "\n",
    "# Example usage\n",
    "docs, metadatas = preprocess_text(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a500c-a221-4ff7-892d-f06545543959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605e85b-b434-451c-a04d-5be3afcd1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(docs):\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    return [model.encode(doc) for doc in docs]\n",
    "\n",
    "# call function\n",
    "embeddings = generate_embeddings(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3100352-1552-4a68-abb5-15ce4ce4cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = {\"documents\": docs, \"embeddings\": embeddings, \"metadatas\": metadatas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161b7fc-1b0c-4d92-b1b6-e80cb0ec1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path from where you want to read the JSON\n",
    "file_path = 've'\n",
    "\n",
    "# Read the JSON file and convert it back to a dictionary\n",
    "with open(file_path, 'r') as file:\n",
    "    vector_store = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf285a-6487-40a2-ab62-6451e16d7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(docs):\n",
    "    return [model.encode(doc) for doc in docs]\n",
    "\n",
    "def generate_query_embedding(query):\n",
    "    return model.encode(query)\n",
    "\n",
    "def find_similar(query, vector_store, model, top_n=5):\n",
    "    try:\n",
    "        # Ensure necessary keys are present in vector_store\n",
    "        if not all(key in vector_store for key in ['documents', 'embeddings', 'metadatas']):\n",
    "            raise ValueError(\"Vector store must contain 'documents', 'embeddings', and 'metadatas' keys.\")\n",
    "\n",
    "        # Encode the query\n",
    "        query_embedding = generate_query_embedding(query)\n",
    "        \n",
    "        # Ensure embeddings are of the same dimension\n",
    "        assert len(query_embedding) == len(vector_store['embeddings'][0]), \"Embedding dimensions do not match.\"\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarities = cosine_similarity([query_embedding], vector_store['embeddings'])[0]\n",
    "        \n",
    "        # Get indices of the top N most similar documents\n",
    "        similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        \n",
    "        # Collect the most similar documents, their metadata, and similarity scores\n",
    "        similar_docs = [\n",
    "            vector_store['documents'][i] for i in similar_indices\n",
    "        ]\n",
    "        \n",
    "        return similar_docs\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_answer_gpt4(relevant_documents, question):\n",
    "    # Combine relevant documents into a single context\n",
    "    context = \"\\n\\n\".join(relevant_documents)\n",
    "    \n",
    "    # Create a prompt for OpenAI\n",
    "    prompt = f\"Based on the following documents, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    # Generate the response from OpenAI using the chat endpoint\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "def answer_question(query, vector_store, model, top_n=1):\n",
    "    # Find similar documents\n",
    "    similar_docs = find_similar(query, vector_store, model, top_n)\n",
    "    \n",
    "    # Generate and return an answer\n",
    "    answer = generate_answer_gpt4(similar_docs, query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28621638-f4f1-4b3f-be03-d8abf81f5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"What are the visa requirements for international students?\"\n",
    "\n",
    "# Get the answer\n",
    "answer = answer_question(query, vector_store, model)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea2e6-8c1f-490b-80fa-c19c2834891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#    while True:\n",
    "#         # Prompt user for input\n",
    "#         query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        \n",
    "#         if query.lower() == 'exit':\n",
    "#             print(\"Exiting the program.\")\n",
    "#             break\n",
    "        \n",
    "#         # Get the answer\n",
    "#         answer = answer_question(query, vector_store, model)\n",
    "#         print(f\"Answer: {answer}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42398bc4-9ca3-43dc-a1af-368dc712ac27",
   "metadata": {},
   "source": [
    "### RGU CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a130ae-41ad-44c9-a151-19c3a77ee213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def generate_embeddings(docs):\n",
    "    return [model.encode(doc) for doc in docs]\n",
    "\n",
    "def generate_query_embedding(query):\n",
    "    return model.encode(query)\n",
    "\n",
    "def find_similar(query, vector_store, model, top_n=1):\n",
    "    try:\n",
    "        # Ensure necessary keys are present in vector_store\n",
    "        if not all(key in vector_store for key in ['documents', 'embeddings', 'metadatas']):\n",
    "            raise ValueError(\"Vector store must contain 'documents', 'embeddings', and 'metadatas' keys.\")\n",
    "\n",
    "        # Encode the query\n",
    "        query_embedding = generate_query_embedding(query)\n",
    "        \n",
    "        # Ensure embeddings are of the same dimension\n",
    "        assert len(query_embedding) == len(vector_store['embeddings'][0]), \"Embedding dimensions do not match.\"\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarities = cosine_similarity([query_embedding], vector_store['embeddings'])[0]\n",
    "        \n",
    "        # Get indices of the top N most similar documents\n",
    "        similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        \n",
    "        # Collect the most similar documents\n",
    "        similar_docs = [\n",
    "            vector_store['documents'][i] for i in similar_indices\n",
    "        ]\n",
    "        \n",
    "        return similar_docs\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_answer_gpt4(relevant_documents, question):\n",
    "    # Combine relevant documents into a single context\n",
    "    context = \"\\n\\n\".join(relevant_documents)\n",
    "    \n",
    "    # Create a prompt for OpenAI\n",
    "    prompt = f\"Based on the following documents, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    # Generate the response from OpenAI using the chat endpoint\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",  # Using the chat model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "def answer_question(query, vector_store, model, top_n=1):\n",
    "    # Define custom responses for specific queries\n",
    "    greetings = [\"hello\", \"hi\", \"greetings\", \"hey\", \"welcome\"]\n",
    "    if any(greeting in query.lower() for greeting in greetings):\n",
    "        return \"Welcome to Robert Gordon University! How can I assist you today?\"\n",
    "\n",
    "    # Find similar documents\n",
    "    similar_docs = find_similar(query, vector_store, model, top_n)\n",
    "    \n",
    "    # Generate and return an answer\n",
    "    answer = generate_answer_gpt4(similar_docs, query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387e2a3-6547-4a7f-a4f0-1ebf833acb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    while True:\n",
    "        # Prompt user for input\n",
    "        query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        \n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Exiting the program.\")\n",
    "            break\n",
    "        \n",
    "        # Get the answer\n",
    "        answer = answer_question(query, vector_store, model)\n",
    "        print(f\"Answer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6429e8-e487-4a40-a38e-783c4e802fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
