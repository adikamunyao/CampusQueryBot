{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26680fda-e099-4e4e-8894-2288f1ed9a06",
   "metadata": {},
   "source": [
    "## QA Model Implementation\n",
    "This project involves building a Question-Answering (QA) system that efficiently retrieves relevant documents from a vector store and generates context-aware answers using GPT-4. The system is designed to maintain conversation history for contextual responses and includes an option to end the conversation when a specific keyword (\"quit\") is detected.\n",
    "\n",
    "### Key Steps in the Implementation:\n",
    "1. SentenceTransformer Model:\n",
    "\n",
    ">The SentenceTransformer model (paraphrase-MiniLM-L6-v2) is used to generate dense vector embeddings from text inputs. These embeddings capture the semantic meaning of sentences, making them suitable for similarity searches.\n",
    "\n",
    "2. Dimensionality Reduction with PCA:\n",
    "\n",
    ">Principal Component Analysis (PCA) is applied to the generated embeddings to reduce their dimensionality. This step is crucial for optimizing memory usage and improving the speed of similarity searches. The reduced embeddings are stored back into the vector store.\n",
    "\n",
    "3. FAISS Indexing for Efficient Search:\n",
    "\n",
    ">A FAISS (Facebook AI Similarity Search) index is built using the reduced embeddings. FAISS enables fast and scalable similarity searches, allowing the system to quickly retrieve the most relevant documents based on a query.\n",
    "\n",
    "4. Query Processing and Search Function:\n",
    "\n",
    ">The system includes a search function that encodes a user query into an embedding, applies PCA for dimensionality reduction, and searches the FAISS index to find the closest matching documents. The top k results are returned for further processing.\n",
    "\n",
    "4. Answer Generation with GPT-4:\n",
    "\n",
    ">The most relevant documents retrieved from the vector store are passed to the GPT-4 model, along with the userâ€™s query, to generate a detailed and context-aware answer. The GPT-4 model synthesizes the information from the documents to provide a coherent response.\n",
    "\n",
    "5. Maintaining Conversation Context:\n",
    "\n",
    ">The system stores the conversation history, including all previous queries and answers, to maintain context throughout the interaction. This history is included in the prompt sent to GPT-4, enabling the model to generate responses that are consistent with the ongoing conversation.\n",
    "\n",
    "6. Conversation Termination:\n",
    "\n",
    ">The system includes a mechanism to end the conversation gracefully. If the user types \"quit\", the conversation loop is terminated, and the system exits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a19843-58b0-4663-95df-6385edf1d469",
   "metadata": {},
   "source": [
    "## setup tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21abacad-ea0a-4623-a273-a6680bbf6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant libraies\n",
    "import os\n",
    "import openai\n",
    "import faiss\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Load OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OpenAI API key is not set in the environment variables.\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Load vector store from disk\n",
    "with open('../data/vector_stored.pkl', 'rb') as f:\n",
    "    vector_store = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Load your SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "embeddings_np = np.array(vector_store['embeddings'])\n",
    "pca = PCA(n_components=100)  # Adjust the number of components as needed\n",
    "embeddings_reduced = pca.fit_transform(embeddings_np)\n",
    "vector_store['embeddings_reduced'] = embeddings_reduced\n",
    "\n",
    "d = embeddings_reduced.shape[1]  # Dimensionality of the reduced embeddings\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance for exact search\n",
    "index.add(embeddings_reduced)  # Add the reduced embeddings to the index\n",
    "\n",
    "\n",
    "class QAChatAgent:\n",
    "    def __init__(self, model, pca, index, vector_store, top_k=5):\n",
    "        self.model = model\n",
    "        self.pca = pca\n",
    "        self.index = index\n",
    "        self.vector_store = vector_store\n",
    "        self.top_k = top_k\n",
    "        self.conversation_history = []  # To store the conversation history\n",
    "\n",
    "    def add_to_history(self, query, answer):\n",
    "        self.conversation_history.append({\"query\": query, \"answer\": answer})\n",
    "\n",
    "    def generate_answer_gpt4(self, relevant_documents, question):\n",
    "        context = \"\\n\\n\".join([f\"User: {entry['query']}\\nAssistant: {entry['answer']}\" for entry in self.conversation_history])\n",
    "        context += \"\\n\\n\" + \"\\n\\n\".join(relevant_documents)\n",
    "        prompt = f\"Based on the following documents and conversation history, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=350,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message['content'].strip()\n",
    "\n",
    "    def search_vector_store(self, query):\n",
    "        query_embedding = self.model.encode(query)\n",
    "        query_embedding_reduced = self.pca.transform([query_embedding]).astype('float32')\n",
    "        D, I = self.index.search(query_embedding_reduced, self.top_k)\n",
    "        results = [(self.vector_store['documents'][i], self.vector_store['metadatas'][i], D[0][n]) for n, i in enumerate(I[0])]\n",
    "        return results\n",
    "\n",
    "    def process_query(self, query):\n",
    "        if query.lower().strip() == \"quit\":\n",
    "            print(\"Ending conversation. Goodbye!\")\n",
    "            return None\n",
    "        \n",
    "        results = self.search_vector_store(query)\n",
    "        relevant_documents = [doc for doc, _, _ in results]\n",
    "        answer = self.generate_answer_gpt4(relevant_documents, query)\n",
    "        \n",
    "        # Add the query and answer to the conversation history\n",
    "        self.add_to_history(query, answer)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "\n",
    "# Usage\n",
    "qa_agent = QAChatAgent(model, pca, index, vector_store)\n",
    "\n",
    "# Simulate conversation loop\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    response = qa_agent.process_query(user_query)\n",
    "    \n",
    "    if response is None:\n",
    "        break  # Exit the loop if \"quit\" is typed\n",
    "    \n",
    "    print(f\"rgu_bot: {response}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962860d-5d27-4bb8-823e-9890eeb1b66e",
   "metadata": {},
   "source": [
    "### load openAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a14edca-8af4-4d20-b1c6-f36e3125f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Load OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OpenAI API key is not set in the environment variables.\")\n",
    "openai.api_key = api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92108d9-f7bb-4ef4-aafc-75770e2dad8a",
   "metadata": {},
   "source": [
    "### load the website content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb881a6-3e2c-4af3-8d9a-af38bf32ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load vector store from disk\n",
    "with open('../data/vector_stored.pkl', 'rb') as f:\n",
    "    vector_stored = pickle.load(f)\n",
    "\n",
    "vector_store = vector_stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b194a4-4e78-439d-a52d-90c1b9cef42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "embeddings_np = np.array(vector_store['embeddings'])\n",
    "pca = PCA(n_components=100)  # Adjust the number of components as needed\n",
    "embeddings_reduced = pca.fit_transform(embeddings_np)\n",
    "vector_store['embeddings_reduced'] = embeddings_reduced\n",
    "\n",
    "d = embeddings_reduced.shape[1]  # Dimensionality of the reduced embeddings\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance for exact search\n",
    "index.add(embeddings_reduced)  # Add the reduced embeddings to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829c4f04-e197-4afd-951d-5a51699b9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAChatAgent:\n",
    "    def __init__(self, model, pca, index, vector_store, top_k=5):\n",
    "        self.model = model\n",
    "        self.pca = pca\n",
    "        self.index = index\n",
    "        self.vector_store = vector_store\n",
    "        self.top_k = top_k\n",
    "        self.conversation_history = []  # To store the conversation history\n",
    "\n",
    "    def add_to_history(self, query, answer):\n",
    "        self.conversation_history.append({\"query\": query, \"answer\": answer})\n",
    "\n",
    "    def generate_answer_gpt4(self, relevant_documents, question):\n",
    "        context = \"\\n\\n\".join([f\"User: {entry['query']}\\nAssistant: {entry['answer']}\" for entry in self.conversation_history])\n",
    "        context += \"\\n\\n\" + \"\\n\\n\".join(relevant_documents)\n",
    "        prompt = f\"Based on the following documents and conversation history, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=350,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message['content'].strip()\n",
    "\n",
    "    def search_vector_store(self, query):\n",
    "        query_embedding = self.model.encode(query)\n",
    "        query_embedding_reduced = self.pca.transform([query_embedding]).astype('float32')\n",
    "        D, I = self.index.search(query_embedding_reduced, self.top_k)\n",
    "        results = [(self.vector_store['documents'][i], self.vector_store['metadatas'][i], D[0][n]) for n, i in enumerate(I[0])]\n",
    "        return results\n",
    "\n",
    "    def process_query(self, query):\n",
    "        if query.lower().strip() == \"quit\":\n",
    "            print(\"Ending conversation. Goodbye!\")\n",
    "            return None\n",
    "        \n",
    "        results = self.search_vector_store(query)\n",
    "        relevant_documents = [doc for doc, _, _ in results]\n",
    "        answer = self.generate_answer_gpt4(relevant_documents, query)\n",
    "        \n",
    "        # Add the query and answer to the conversation history\n",
    "        self.add_to_history(query, answer)\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e24fbb-9dc0-4a8c-abf8-7c670bb1f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgu_bot: Hello! How can I assist you today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  who are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgu_bot: I am an AI assistant here to help you with any questions or tasks you have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "qa_agent = QAChatAgent(model, pca, index, vector_store)\n",
    "\n",
    "# Simulate conversation loop\n",
    "while True:\n",
    "    user_query = input(\"You: \")\n",
    "    response = qa_agent.process_query(user_query)\n",
    "    \n",
    "    if response is None:\n",
    "        break  # Exit the loop if \"quit\" is typed\n",
    "    \n",
    "    print(f\"rgu_bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d804f7-1b48-4890-9b49-f8eaf3acd45c",
   "metadata": {},
   "source": [
    "> <b>This implementation combines efficient vector search techniques with the powerful natural language generation capabilities of GPT-4, resulting in a robust and responsive QA system capable of handling complex user queries with contextually relevant answers.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b6229-50ca-40ee-af0a-826af1bd7dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot_env",
   "language": "python",
   "name": "bot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
