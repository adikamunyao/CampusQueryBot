{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5436ba58-64c4-4aef-9424-35402a767935",
   "metadata": {},
   "source": [
    "# 1. Project Overview\n",
    "The primary goal of this project is to build an efficient and user-friendly chatbot that can handle various queries about RGU, ranging from visa requirements for international students to general campus information. By integrating advanced Natural Language Processing (NLP) techniques, we have developed a system capable of understanding and responding to user inquiries with high accuracy.\n",
    "\n",
    "### Skills and Technologies Used\n",
    "1. Natural Language Processing (NLP): We used the SentenceTransformer library to generate and compare text embeddings. This allows the chatbot to understand and match user queries with relevant documents.\n",
    "\n",
    "2. Vector Databases: The project involves creating and managing vector stores for document embeddings. This is crucial for efficient and scalable query processing.\n",
    "\n",
    "3. OpenAI GPT-4: We integrated OpenAI’s GPT-4 model to generate contextually relevant answers based on the retrieved documents. This step enhances the quality of responses and ensures they are coherent and informative.\n",
    "\n",
    "4. Error Handling and Validation: Implementing robust error handling and validation checks ensures that the chatbot operates smoothly and handles unexpected inputs gracefully.\n",
    "\n",
    "5. Environment Configuration: Utilized environment variables and configuration management to securely handle API keys and other sensitive information.\n",
    "\n",
    "## Evolution of AI in Chatbots\n",
    "The field of AI and NLP has seen significant advancements in recent years. Modern chatbots are no longer limited to keyword-based responses but can understand context and nuance, thanks to models like GPT-4. These models are trained on vast amounts of data, enabling them to generate responses that are not only accurate but also human-like.\n",
    "\n",
    "The Project exemplifies the application of these advancements. By combining pre-trained language models with custom embeddings, we have created a system that evolves with user interactions and continuously improves its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b946ad-6369-41c8-a9be-f846a13d057a",
   "metadata": {},
   "source": [
    "# 2. Relevant Libraries\n",
    "<p>\n",
    "This set of imports indicates that the code is designed to work with various aspects of natural language processing (NLP), including interacting with APIs, handling and processing text data, and performing similarity analysis. The libraries imported facilitate tasks such as making HTTP requests, parsing and handling data, and utilizing advanced models for generating embeddings and performing similarity comparisons.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dbdca-af9b-4aa5-817b-55438179b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import requests\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0dd6a8-7e3e-4e77-b0ef-f98cb5a67a57",
   "metadata": {},
   "source": [
    "1. **`import os`**\n",
    "   - **Purpose:** Provides a way to interact with the operating system. It allows you to perform operations such as reading or writing to the file system, handling environment variables, and more.\n",
    "   - **Common Use Cases:** Accessing environment variables, manipulating file paths, and executing system commands.\n",
    "\n",
    "2. **`import openai`**\n",
    "   - **Purpose:** This is the official OpenAI Python client library for interacting with OpenAI's API services.\n",
    "   - **Common Use Cases:** Making API calls to access OpenAI's models for tasks such as text generation, completion, or embeddings.\n",
    "\n",
    "3. **`import requests`**\n",
    "   - **Purpose:** A popular library for making HTTP requests in Python. It simplifies sending HTTP requests and handling responses.\n",
    "   - **Common Use Cases:** Fetching data from APIs or web services, downloading files, and handling HTTP responses.\n",
    "\n",
    "4. **`import numpy as np`**\n",
    "   - **Purpose:** A fundamental library for numerical computations in Python. It provides support for large multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays.\n",
    "   - **Common Use Cases:** Handling arrays, performing mathematical operations, and working with large datasets.\n",
    "\n",
    "5. **`import xmltodict`**\n",
    "   - **Purpose:** A library for converting XML data into Python dictionaries, making it easier to work with XML data.\n",
    "   - **Common Use Cases:** Parsing XML files or responses and converting them into a more accessible dictionary format for further processing.\n",
    "\n",
    "6. **`from bs4 import BeautifulSoup`**\n",
    "   - **Purpose:** BeautifulSoup is a library for parsing HTML and XML documents. It provides methods for navigating and modifying the parse tree.\n",
    "   - **Common Use Cases:** Web scraping, extracting data from HTML or XML, and cleaning up data for further analysis.\n",
    "\n",
    "7. **`from dotenv import load_dotenv`**\n",
    "   - **Purpose:** Loads environment variables from a `.env` file into the environment. This is useful for managing configuration settings and sensitive information.\n",
    "   - **Common Use Cases:** Loading API keys, database credentials, or other environment-specific configurations.\n",
    "\n",
    "8. **`from openai.embeddings_utils import get_embedding`**\n",
    "   - **Purpose:** Provides utilities for working with embeddings generated by OpenAI's models. The `get_embedding` function retrieves embeddings for text inputs.\n",
    "   - **Common Use Cases:** Generating and using text embeddings for natural language processing tasks, such as similarity analysis or text classification.\n",
    "\n",
    "9. **`from sentence_transformers import SentenceTransformer`**\n",
    "   - **Purpose:** A library for generating sentence embeddings using pre-trained models. It helps in obtaining dense vector representations of sentences or documents.\n",
    "   - **Common Use Cases:** Semantic textual similarity, clustering, and retrieval tasks by converting sentences into vectors.\n",
    "\n",
    "10. **`from langchain.text_splitter import CharacterTextSplitter`**\n",
    "    - **Purpose:** Provides a utility for splitting text into smaller chunks based on character length. This is useful for processing large texts by breaking them into manageable parts.\n",
    "    - **Common Use Cases:** Text preprocessing, document chunking for analysis, and handling long texts in NLP tasks.\n",
    "\n",
    "11. **`from sklearn.metrics.pairwise import cosine_similarity`**\n",
    "    - **Purpose:** Provides functions to compute pairwise similarity measures between vectors, specifically cosine similarity.\n",
    "    - **Common Use Cases:** Measuring similarity between text embeddings or feature vectors, clustering, and information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03619e5d-7b2b-47ef-af41-63b5fa95d16a",
   "metadata": {},
   "source": [
    "### Setting Up the OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd262896-0b63-4ef3-8c96-4496444f1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d6fc2-e4ac-4832-a7d7-752f067424e5",
   "metadata": {},
   "source": [
    "- **Purpose:** This line uses the `load_dotenv` function from the `dotenv` library to load environment variables from a `.env` file into the environment.\n",
    "- **Explanation:** The `.env` file typically contains configuration settings and sensitive information such as API keys, which should not be hardcoded into your source code. By loading these variables from the `.env` file, you can manage your configuration separately and securely. This approach keeps sensitive information like API keys out of the source code repository.\n",
    "\n",
    "- **Purpose:** This code snippet sets the API key for the OpenAI client library.\n",
    "- **Explanation:**\n",
    "  - `os.getenv(\"OPENAI_API_KEY\")`: Retrieves the value of the environment variable named `OPENAI_API_KEY`. The `os.getenv` function is used to access environment variables, which were loaded into the environment by `load_dotenv()`.\n",
    "  - `openai.api_key = api_key`: Assigns the retrieved API key to the `api_key` attribute of the `openai` library. This sets up authentication for making API calls to OpenAI's services. By setting this attribute, you ensure that all subsequent interactions with OpenAI’s API will use the provided API key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ff9b0-4b09-47c5-b985-fcfbdf1ab716",
   "metadata": {},
   "source": [
    "> By keeping sensitive information in environment variables and not hardcoding them into your source code, you enhance the security and maintainability of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271011c-b0d4-4b29-a2eb-01ece82aec28",
   "metadata": {},
   "source": [
    "# 3. Web Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cbbc3-d029-4073-99e0-0e81e782cff5",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to extract and process text from web pages and sitemaps using web scraping techniques. This code showcases practical applications of web scraping skills for retrieving, parsing, and processing web data. By combining HTTP requests, HTML and XML parsing, and text processing, the code effectively extracts relevant content from web pages and sitemaps, enabling targeted data analysis and processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fee919-0fb3-4c10-b103-115b9b4d20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    return '\\n'.join(line for line in lines if line)\n",
    "\n",
    "def fetch_sitemap(url):\n",
    "    r = requests.get(url)\n",
    "    xml = r.text\n",
    "    raw = xmltodict.parse(xml)\n",
    "    return raw\n",
    "\n",
    "def get_relevant_pages(sitemap, keyword):\n",
    "    pages = []\n",
    "    for info in sitemap['urlset']['url']:\n",
    "        url = info['loc']\n",
    "        if keyword in url:\n",
    "            pages.append({'text': extract_text_from(url), 'source': url})\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd808fec-958a-4d99-a349-63378039c699",
   "metadata": {},
   "source": [
    "### **3.1. `extract_text_from(url)` Function**\n",
    "\n",
    "```python\n",
    "def extract_text_from(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    return '\\n'.join(line for line in lines if line)\n",
    "```\n",
    "\n",
    "- **Purpose:** This function extracts and cleans the text content from a given URL.\n",
    "- **Process:**\n",
    "  - `requests.get(url).text`: Sends an HTTP GET request to the specified URL and retrieves the HTML content of the page.\n",
    "  - `BeautifulSoup(html, features=\"html.parser\")`: Parses the HTML content using BeautifulSoup, a library for web scraping and parsing HTML/XML documents.\n",
    "  - `soup.get_text()`: Extracts all the text content from the HTML, stripping away any HTML tags.\n",
    "  - `text.splitlines()`: Splits the extracted text into lines.\n",
    "  - `(line.strip() for line in text.splitlines())`: Strips leading and trailing whitespace from each line.\n",
    "  - `'\\n'.join(line for line in lines if line)`: Joins the non-empty lines into a single string, separated by newline characters.\n",
    "- **Necessity:** This function is essential for web scraping as it converts raw HTML content into clean, usable text. This is particularly useful for further processing or analysis of the page content.\n",
    "\n",
    "#### **3.2. `fetch_sitemap(url)` Function**\n",
    "\n",
    "```python\n",
    "def fetch_sitemap(url):\n",
    "    r = requests.get(url)\n",
    "    xml = r.text\n",
    "    raw = xmltodict.parse(xml)\n",
    "    return raw\n",
    "```\n",
    "\n",
    "- **Purpose:** This function retrieves and parses the XML sitemap from a given URL.\n",
    "- **Process:**\n",
    "  - `requests.get(url)`: Sends an HTTP GET request to the URL of the sitemap.\n",
    "  - `r.text`: Retrieves the XML content of the sitemap.\n",
    "  - `xmltodict.parse(xml)`: Parses the XML content into a Python dictionary using `xmltodict`, which simplifies the XML structure into a dictionary format.\n",
    "- **Necessity:** Sitemaps are used to inform search engines and users about the structure of a website. Fetching and parsing a sitemap allows you to programmatically access the URLs listed, which can be useful for tasks like site crawling and content extraction.\n",
    "\n",
    "#### **3.3. `get_relevant_pages(sitemap, keyword)` Function**\n",
    "\n",
    "```python\n",
    "def get_relevant_pages(sitemap, keyword):\n",
    "    pages = []\n",
    "    for info in sitemap['urlset']['url']:\n",
    "        url = info['loc']\n",
    "        if keyword in url:\n",
    "            pages.append({'text': extract_text_from(url), 'source': url})\n",
    "    return pages\n",
    "```\n",
    "\n",
    "- **Purpose:** This function filters the URLs from a sitemap based on a keyword and extracts text from the relevant pages.\n",
    "- **Process:**\n",
    "  - Iterates over each URL entry in the parsed sitemap dictionary.\n",
    "  - `info['loc']`: Extracts the URL from each sitemap entry.\n",
    "  - `if keyword in url`: Checks if the given keyword is present in the URL.\n",
    "  - `extract_text_from(url)`: Calls the previously defined function to extract and clean the text content from the relevant pages.\n",
    "  - `pages.append({'text': extract_text_from(url), 'source': url})`: Stores the extracted text and URL in a list for each relevant page.\n",
    "- **Necessity:** This function helps in filtering and processing web pages based on specific criteria (e.g., a keyword in the URL). This is useful for targeted data extraction, such as collecting content related to a particular topic or ensuring that only relevant pages are processed.\n",
    "\n",
    "### 3.4 How Web Scraping Skills Have Been Used\n",
    "\n",
    "3.4.1. **Data Retrieval:**\n",
    "   - The `requests` library is used to fetch HTML and XML content from the web. This involves sending HTTP requests and handling the responses, which is fundamental to web scraping.\n",
    "\n",
    "3.4.2. **HTML Parsing:**\n",
    "   - `BeautifulSoup` is employed to parse HTML documents. It extracts meaningful text from the raw HTML, allowing you to navigate and manipulate the HTML structure.\n",
    "\n",
    "3.4.3. **XML Parsing:**\n",
    "   - `xmltodict` is used to parse XML sitemaps into dictionaries. This conversion simplifies the process of accessing and processing structured data in XML format.\n",
    "\n",
    "3.4.4. **Text Processing:**\n",
    "   - The `extract_text_from` function cleans and formats text extracted from HTML. This step is crucial for ensuring that the data is in a usable format for further analysis or processing.\n",
    "\n",
    "3.4.5. **Content Filtering:**\n",
    "   - The `get_relevant_pages` function demonstrates filtering and processing based on specific criteria, showcasing how to selectively handle and extract content based on keywords or other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460799b-1ada-4d65-9015-0853ccaf50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function \n",
    "sitemap_url = \"https://www.rgu.ac.uk/index.php?option=com_jmap&view=sitemap&format=xml\"\n",
    "sitemap = fetch_sitemap(sitemap_url)\n",
    "pages = get_relevant_pages(sitemap, 'international-students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cc439-986d-4b1f-b24e-957a2dfc9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5052f-6ffc-494a-bbd7-9050cfeafb74",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing\n",
    "The preprocess_text function is designed to preprocess and organize text data by splitting it into smaller chunks and associating metadata with each chunk. This approach facilitates more efficient text handling and analysis, making it easier to work with large volumes of textual data in NLP applications. The use of CharacterTextSplitter from the langchain library is a practical solution for managing text data in a scalable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0ffe8-1537-48a7-ba54-deabf9bf85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(pages):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
    "    docs, metadatas = [], []\n",
    "    for page in pages:\n",
    "        splits = text_splitter.split_text(page['text'])\n",
    "        docs.extend(splits)\n",
    "        metadatas.extend([{\"source\": page['source']}] * len(splits))\n",
    "    return docs, metadatas\n",
    "\n",
    "# Example usage\n",
    "docs, metadatas = preprocess_text(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e16fa4-af6c-4d4f-9e52-f68bfb77d547",
   "metadata": {},
   "source": [
    "\n",
    "### **4.1. `preprocess_text(pages)` Function**\n",
    "\n",
    "- **Purpose:** This function processes and splits text content from a list of web pages into manageable chunks and creates metadata associated with each chunk. It prepares the text data for further analysis or use in natural language processing (NLP) tasks.\n",
    "\n",
    "- **Process:**\n",
    "  - **Text Splitter Initialization:**\n",
    "    ```python\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
    "    ```\n",
    "    - **`CharacterTextSplitter`**: This is a utility from the `langchain` library that splits text into chunks based on a specified character length and separator. Here, it's configured to split text into chunks of up to 1500 characters, separated by newline characters.\n",
    "    - **Reason for Chunking:** Splitting large text into smaller chunks helps in processing and analyzing text more efficiently. It ensures that text chunks are manageable in size and easier to handle for various NLP tasks.\n",
    "\n",
    "  - **Initialize Lists:**\n",
    "    ```python\n",
    "    docs, metadatas = [], []\n",
    "    ```\n",
    "    - **`docs`**: A list that will store the split text chunks.\n",
    "    - **`metadatas`**: A list that will store metadata associated with each text chunk.\n",
    "\n",
    "  - **Process Each Page:**\n",
    "    ```python\n",
    "    for page in pages:\n",
    "        splits = text_splitter.split_text(page['text'])\n",
    "        docs.extend(splits)\n",
    "        metadatas.extend([{\"source\": page['source']}] * len(splits))\n",
    "    ```\n",
    "    - **Iterate Over Pages:** For each page in the `pages` list, extract and process the text.\n",
    "    - **Split Text:** `text_splitter.split_text(page['text'])` splits the text into chunks based on the specified chunk size and separator.\n",
    "    - **Extend Lists:**\n",
    "      - **`docs.extend(splits)`**: Adds the split text chunks to the `docs` list.\n",
    "      - **`metadatas.extend([{\"source\": page['source']}] * len(splits))`**: Adds metadata entries to the `metadatas` list. Each metadata entry contains the source URL of the page and is repeated for each text chunk generated from that page.\n",
    "\n",
    "\n",
    "### 4.2. Importance and Benefits\n",
    "\n",
    "4.2.1. **Efficient Text Handling:**\n",
    "   - **Chunking:** By splitting text into smaller chunks, you improve the efficiency of processing and analysis. Large texts can be unwieldy and difficult to handle in a single operation.\n",
    "\n",
    "4.2.2. **Enhanced Search and Retrieval:**\n",
    "   - **Metadata Association:** Associating metadata with each text chunk allows you to trace back the source of the information, which is valuable for tracking and analyzing data sources.\n",
    "\n",
    "4.2.3. **Improved NLP Tasks:**\n",
    "   - **Manageable Size:** Smaller chunks are easier to work with in various NLP tasks, such as text classification, clustering, or similarity analysis.\n",
    "\n",
    "4.2.4. **Scalability:**\n",
    "   - **Chunking Text:** Helps in scaling the processing pipeline, allowing for handling larger volumes of text efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a500c-a221-4ff7-892d-f06545543959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c4432-d8f4-4c3f-8bb1-7af063bafb60",
   "metadata": {},
   "source": [
    "### 4.3 Embeddings\n",
    "Embeddings are a fundamental component in NLP that translate text into numerical vectors, capturing semantic meaning and enabling more efficient and effective text processing. For a chatbot, embeddings enhance understanding, response generation, and similarity search capabilities. The provided code uses the `SentenceTransformer` model to generate embeddings for a list of documents, which is a crucial step in building an intelligent and responsive chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e981e-a935-4010-9b4b-b97ec1bf21e6",
   "metadata": {},
   "source": [
    "#### **4.3.1. What Are Embeddings?**\n",
    "\n",
    "Embeddings are numerical vector representations of text data. They capture semantic meaning in a dense vector space, where similar texts have vectors that are close to each other. Embeddings are essential in natural language processing (NLP) and machine learning for tasks that involve understanding and analyzing text.\n",
    "\n",
    "#### **4.3.2. Why Are Embeddings Necessary?**\n",
    "\n",
    "In NLP tasks, embeddings are crucial because they provide a way to translate textual information into a format that machine learning models can process effectively. They are necessary for several reasons:\n",
    "\n",
    "1. **Semantic Understanding:**\n",
    "   - Embeddings capture the semantic meaning of words, phrases, or documents. They enable models to understand and compare the meaning of different texts, even if the exact wording is different.\n",
    "\n",
    "2. **Dimensionality Reduction:**\n",
    "   - Text data, when converted to embeddings, is represented in a lower-dimensional space compared to raw text. This reduces the computational complexity and storage requirements, making it easier to work with large datasets.\n",
    "\n",
    "3. **Improved Performance:**\n",
    "   - Embeddings enhance the performance of machine learning models by providing a more informative and nuanced representation of text. They allow models to better capture relationships and similarities between texts.\n",
    "\n",
    "4. **Similarity Measurement:**\n",
    "   - Embeddings facilitate the measurement of similarity between texts. By comparing embeddings, you can determine how similar different pieces of text are to each other, which is useful for various tasks such as search and retrieval, clustering, and recommendation.\n",
    "\n",
    "#### **4.3.3. How Do Embeddings Help the Chatbot?**\n",
    "\n",
    "For a chatbot, embeddings are particularly useful in several ways:\n",
    "\n",
    "1. **Enhanced Understanding:**\n",
    "   - Embeddings enable the chatbot to understand and interpret user queries more effectively by capturing the semantic meaning of the text. This helps in generating more relevant and accurate responses.\n",
    "\n",
    "2. **Improved Response Generation:**\n",
    "   - With embeddings, the chatbot can match user queries to the most relevant responses or information from its knowledge base. This improves the quality of the responses and the overall user experience.\n",
    "\n",
    "3. **Contextual Matching:**\n",
    "   - Embeddings allow the chatbot to handle variations in user input by mapping similar queries to the same or related responses. This helps in managing different phrasings and synonyms.\n",
    "\n",
    "4. **Similarity Search:**\n",
    "   - The chatbot can use embeddings to perform similarity searches within a knowledge base or document repository. By comparing embeddings, it can retrieve information that closely matches the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605e85b-b434-451c-a04d-5be3afcd1840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(docs):\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    return [model.encode(doc) for doc in docs]\n",
    "\n",
    "# call function\n",
    "embeddings = generate_embeddings(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63b4d3-a0da-4e70-a336-264a06613486",
   "metadata": {},
   "source": [
    "#### **Code Explanation**\n",
    "\n",
    "```python\n",
    "def generate_embeddings(docs):\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    return [model.encode(doc) for doc in docs]\n",
    "\n",
    "# call function\n",
    "embeddings = generate_embeddings(docs)\n",
    "```\n",
    "\n",
    "- **Function Definition:**\n",
    "  ```python\n",
    "  def generate_embeddings(docs):\n",
    "      model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "      return [model.encode(doc) for doc in docs]\n",
    "  ```\n",
    "  - **`SentenceTransformer('paraphrase-MiniLM-L6-v2')`:** Initializes a pre-trained Sentence Transformer model that generates embeddings. The `'paraphrase-MiniLM-L6-v2'` model is optimized for capturing the semantic similarity between sentences.\n",
    "  - **`model.encode(doc)`:** Converts each document (`doc`) into its vector representation (embedding). This is done for all documents in the `docs` list.\n",
    "\n",
    "- **Function Call:**\n",
    "  ```python\n",
    "  embeddings = generate_embeddings(docs)\n",
    "  ```\n",
    "  - **Purpose:** Calls the `generate_embeddings` function with the list of documents (`docs`) to generate their embeddings. The result is a list of embeddings, where each embedding corresponds to a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c6257-82a8-46df-871c-8d4ea5940790",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_store_json_compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b865c9c-1d6c-4af2-b044-a5b439b6f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3100352-1552-4a68-abb5-15ce4ce4cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectore store\n",
    "vector_store = {\"documents\": docs, \"embeddings\": embeddings, \"metadatas\": metadatas}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0287afa-f192-4cd7-a5e4-00b901cf882a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'vector_store.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector_store.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read the JSON file and convert it back to a dictionary\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m     vector_store \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vector_store.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Specify the file path from where you want to read the JSON\n",
    "file_path = 'vector_store.json'\n",
    "\n",
    "# Read the JSON file and convert it back to a dictionary\n",
    "with open(file_path, 'r') as file:\n",
    "    vector_store = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1189453a-78bc-4644-ac33-4fecf8523e0f",
   "metadata": {},
   "source": [
    "### 4.4 vector_store\n",
    "`vector_store` is a dictionary that organizes and stores three key pieces of information.\n",
    "\n",
    ">The `vector_store` dictionary efficiently organizes and stores text data, embeddings, and metadata in a structured format. This organization facilitates various NLP tasks, including similarity searches and information retrieval, by providing quick access to text chunks, their semantic representations, and associated metadata. This setup is essential for building effective and responsive systems, such as chatbots, that rely on textual data and semantic understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b1718-1697-4739-ac96-aee3c55acc7d",
   "metadata": {},
   "source": [
    "4.4.1. **Documents:**\n",
    "   - **Key:** `\"documents\"`\n",
    "   - **Value:** `docs` (a list of text chunks obtained from the `preprocess_text` function)\n",
    "   - **Purpose:** This list contains the actual text data split into manageable chunks. These chunks are the raw content that you want to analyze or use for further processing.\n",
    "\n",
    "4.4.2. **Embeddings:**\n",
    "   - **Key:** `\"embeddings\"`\n",
    "   - **Value:** `embeddings` (a list of vector representations of the documents obtained from the `generate_embeddings` function)\n",
    "   - **Purpose:** This list contains the embeddings generated for each text chunk. Embeddings are dense vector representations that capture the semantic meaning of the text, allowing for efficient similarity comparisons and other NLP tasks.\n",
    "\n",
    "4.4.3. **Metadatas:**\n",
    "   - **Key:** `\"metadatas\"`\n",
    "   - **Value:** `metadatas` (a list of metadata dictionaries corresponding to each document, obtained from the `preprocess_text` function)\n",
    "   - **Purpose:** This list contains metadata related to each text chunk, such as the source URL of the document. Metadata helps in tracking the origin of each chunk and can be useful for context or additional information retrieval.\n",
    "\n",
    "#### **Why is `vector_store` Necessary?**\n",
    "\n",
    ">**Organized Storage:**\n",
    "   - **Purpose:** `vector_store` consolidates all relevant information (text chunks, embeddings, and metadata) into a single, organized structure. This makes it easier to manage and access the data for subsequent processing or querying.\n",
    "\n",
    ">**Efficient Retrieval:**\n",
    "   - **Purpose:** By storing documents, embeddings, and metadata together, you can efficiently retrieve and use this information when performing tasks such as similarity searches, data analysis, or generating responses. The structure allows for quick access to the text, its vector representation, and associated metadata.\n",
    "\n",
    ">**Improved Performance:**\n",
    "   - **Purpose:** The separation of text data (documents) and its vector representations (embeddings) supports efficient similarity calculations. For instance, if you want to find the most similar documents to a given query, you can compute the query’s embedding and compare it to the stored embeddings using vector operations.\n",
    "\n",
    ">**Enhanced Functionality:**\n",
    "   - **Purpose:** The metadata provides additional context that can be useful for understanding the origin or additional details about each document. For example, if the chatbot retrieves a document, the metadata might include the source URL or other relevant information that can be presented to the user.\n",
    "\n",
    "\n",
    "The `vector_store` dictionary can be used in various ways, such as:\n",
    "\n",
    "- **Similarity Search:**\n",
    "  - Compute the embedding for a user query and compare it to the stored embeddings to find the most similar documents.\n",
    "  \n",
    "- **Information Retrieval:**\n",
    "  - Retrieve the documents and their associated metadata based on the similarity search results to provide relevant responses or information to the user.\n",
    "\n",
    "- **Contextual Analysis:**\n",
    "  - Use metadata to provide additional context or details about the source of the information being presented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2417606-d7a1-4532-8928-3c13cce96de2",
   "metadata": {},
   "source": [
    "### 4.5 Similarity in Document Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed754d6f-7202-4d9f-8211-bc8828396019",
   "metadata": {},
   "source": [
    "#### Concept of Similarity\n",
    "\n",
    "In the context of document retrieval and natural language processing (NLP), **similarity** refers to how closely two pieces of text (or their vector representations) align with each other. This alignment can be based on various factors such as meaning, context, or content. The core idea is to find documents that are most relevant to a given query or context by comparing how similar their representations are.\n",
    "\n",
    "#### Types of Similarity\n",
    "\n",
    "4.5.1. **Cosine Similarity**: \n",
    "   - **Definition**: Cosine similarity is a metric used to measure how similar two vectors are irrespective of their magnitude. It calculates the cosine of the angle between two vectors in a multi-dimensional space.\n",
    "   - **Usage**: It is widely used in NLP to measure document similarity by comparing the vector embeddings of texts. It is effective for understanding the orientation of vectors (i.e., how similar the text is) rather than their magnitude.\n",
    "\n",
    "4.5.2. **Euclidean Distance**:\n",
    "   - **Definition**: Euclidean distance measures the straight-line distance between two points in multi-dimensional space.\n",
    "   - **Usage**: While less common in NLP for similarity tasks, it can be used for clustering and classification where the actual distance between points is relevant.\n",
    "\n",
    "4.5.3. **Jaccard Similarity**:\n",
    "   - **Definition**: Jaccard similarity measures the similarity between two sets by dividing the size of their intersection by the size of their union.\n",
    "   - **Usage**: This is more common in text analysis for comparing the similarity of sets of words or phrases rather than vector embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7e08e-8ff9-4437-8091-2595fbc333c0",
   "metadata": {},
   "source": [
    "### Application\n",
    "> **Vector Representation**\n",
    "\n",
    "Each document and query are converted into numerical vectors (embeddings) using the SentenceTransformer model. These embeddings capture the semantic meaning of the text, allowing for meaningful comparison.\n",
    "\n",
    "> **Query Encoding**\n",
    "\n",
    "When a user submits a query, it is converted into an embedding using the same model. This ensures that the query and documents are represented in the same space, making comparison feasible.\n",
    "\n",
    "> **Similarity Calculation**\n",
    "\n",
    "The cosine similarity between the query embedding and each document embedding is computed. This process involves:\n",
    "   - **Normalization**: Vectors are normalized to have unit length (magnitude of 1). This ensures that the cosine similarity calculation only considers the angle between vectors, which is a measure of similarity.\n",
    "   - **Dot Product Calculation**: The dot product of the query vector and each document vector is computed.\n",
    "   - **Similarity Score**: The cosine similarity score is derived from the dot product, indicating how closely related the query is to each document.\n",
    "\n",
    "> **Retrieving Top Documents**\n",
    "\n",
    "The documents with the highest similarity scores are selected as the most relevant. These top documents are then used to generate a contextual response to the user query.\n",
    "\n",
    "- **Query Encoding**: The query is converted into an embedding vector.\n",
    "- **Cosine Similarity**: The cosine similarity between the query vector and each document vector is calculated.\n",
    "- **Top Documents**: The documents with the highest similarity scores are selected and returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf285a-6487-40a2-ab62-6451e16d7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def generate_embeddings(docs):\n",
    "    return [model.encode(doc) for doc in docs]\n",
    "\n",
    "def generate_query_embedding(query):\n",
    "    return model.encode(query)\n",
    "\n",
    "def find_similar(query, vector_store, model, top_n=5):\n",
    "    try:\n",
    "        # Ensure necessary keys are present in vector_store\n",
    "        if not all(key in vector_store for key in ['documents', 'embeddings', 'metadatas']):\n",
    "            raise ValueError(\"Vector store must contain 'documents', 'embeddings', and 'metadatas' keys.\")\n",
    "\n",
    "        # Encode the query\n",
    "        query_embedding = generate_query_embedding(query)\n",
    "        \n",
    "        # Ensure embeddings are of the same dimension\n",
    "        assert len(query_embedding) == len(vector_store['embeddings'][0]), \"Embedding dimensions do not match.\"\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarities = cosine_similarity([query_embedding], vector_store['embeddings'])[0]\n",
    "        \n",
    "        # Get indices of the top N most similar documents\n",
    "        similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        \n",
    "        # Collect the most similar documents, their metadata, and similarity scores\n",
    "        similar_docs = [\n",
    "            vector_store['documents'][i] for i in similar_indices\n",
    "        ]\n",
    "        \n",
    "        return similar_docs\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_answer_gpt4(relevant_documents, question):\n",
    "    # Combine relevant documents into a single context\n",
    "    context = \"\\n\\n\".join(relevant_documents)\n",
    "    \n",
    "    # Create a prompt for OpenAI\n",
    "    prompt = f\"Based on the following documents, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "    # Generate the response from OpenAI using the chat endpoint\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "def answer_question(query, vector_store, model, top_n=1):\n",
    "    # Find similar documents\n",
    "    similar_docs = find_similar(query, vector_store, model, top_n)\n",
    "    \n",
    "    # Generate and return an answer\n",
    "    answer = generate_answer_gpt4(similar_docs, query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28621638-f4f1-4b3f-be03-d8abf81f5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"What are the visa requirements for international students?\"\n",
    "\n",
    "# Get the answer\n",
    "answer = answer_question(query, vector_store, model)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a130ae-41ad-44c9-a151-19c3a77ee213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the SentenceTransformer model\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# def generate_embeddings(docs):\n",
    "#     return [model.encode(doc) for doc in docs]\n",
    "\n",
    "# def generate_query_embedding(query):\n",
    "#     return model.encode(query)\n",
    "\n",
    "# def find_similar(query, vector_store, model, top_n=1):\n",
    "#     try:\n",
    "#         # Ensure necessary keys are present in vector_store\n",
    "#         if not all(key in vector_store for key in ['documents', 'embeddings', 'metadatas']):\n",
    "#             raise ValueError(\"Vector store must contain 'documents', 'embeddings', and 'metadatas' keys.\")\n",
    "\n",
    "#         # Encode the query\n",
    "#         query_embedding = generate_query_embedding(query)\n",
    "        \n",
    "#         # Ensure embeddings are of the same dimension\n",
    "#         assert len(query_embedding) == len(vector_store['embeddings'][0]), \"Embedding dimensions do not match.\"\n",
    "        \n",
    "#         # Compute similarity\n",
    "#         similarities = cosine_similarity([query_embedding], vector_store['embeddings'])[0]\n",
    "        \n",
    "#         # Get indices of the top N most similar documents\n",
    "#         similar_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        \n",
    "#         # Collect the most similar documents\n",
    "#         similar_docs = [\n",
    "#             vector_store['documents'][i] for i in similar_indices\n",
    "#         ]\n",
    "        \n",
    "#         return similar_docs\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#         return []\n",
    "\n",
    "# def generate_answer_gpt4(relevant_documents, question):\n",
    "#     # Combine relevant documents into a single context\n",
    "#     context = \"\\n\\n\".join(relevant_documents)\n",
    "    \n",
    "#     # Create a prompt for OpenAI\n",
    "#     prompt = f\"Based on the following document, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    \n",
    "#     # Generate the response from OpenAI using the chat endpoint\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-4o-mini\",  # Using the chat model\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#         ],\n",
    "#         max_tokens=150,\n",
    "#         temperature=0.7\n",
    "#     )\n",
    "    \n",
    "#     return response.choices[0].message['content'].strip()\n",
    "\n",
    "# def answer_question(query, vector_store, model, top_n=5):\n",
    "#     # Define custom responses for specific queries\n",
    "#     greetings = [\"hello\", \"hi\", \"greetings\", \"hey\", \"welcome\"]\n",
    "#     if any(greeting in query.lower() for greeting in greetings):\n",
    "#         return \"Welcome to Robert Gordon University! How can I assist you today?\"\n",
    "\n",
    "#     # Find similar documents\n",
    "#     similar_docs = find_similar(query, vector_store, model, top_n)\n",
    "    \n",
    "#     # Generate and return an answer\n",
    "#     answer = generate_answer_gpt4(similar_docs, query)\n",
    "#     return answer\n",
    "\n",
    "# def main():    \n",
    "#     while True:\n",
    "#         # Prompt user for input\n",
    "#         query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        \n",
    "#         if query.lower() == 'exit':\n",
    "#             print(\"Exiting the program.\")\n",
    "#             break\n",
    "        \n",
    "#         # Get the answer\n",
    "#         answer = answer_question(query, vector_store, model)\n",
    "#         print(f\"Answer: {answer}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5c150-3fea-49d4-a6d3-efccf380603b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project demonstrates the practical application of state-of-the-art AI techniques in creating an intelligent chatbot. It showcases how integrating different technologies and methodologies can result in a powerful tool that enhances user experience and provides valuable information efficiently.\n",
    "\n",
    "Thank you for exploring the RGU Chatbot Project. Feel free to explore the code, contribute, or provide feedback to help us further refine and enhance this tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
